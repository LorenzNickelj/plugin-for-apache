buildscript {
  repositories {
    ivy {
      url "/home/abain/local-repo"
    }
    mavenCentral()
  }

  dependencies {
    classpath group: 'com.linkedin', name: 'hadoop-plugin', version: '0.0.1'
  }
}

apply plugin: HadoopPlugin

def job1 = new PigJob("job1")
job1.script = "src/main/pig/count_by_country.job"
job1.parameters["member_summary"] = "/data/derived/member/summary#LATEST"
job1.parameters["output"] = "count_by_country.dat"
global job1

def train = new AzkabanWorkflow('train')
// global train

def job1Ref = lookup('job1')
job1Ref.script = "src/main/pig/count_by_country.job1"

lookup('job1') {
  job1Ref.script = "src/main/pig/count_by_country.job11"
}

def generateWorkflows() {
  workflow('genTrain') {
  }
}

pigJob('prodJob') {
  script = 'globalProdJob'
}

workflow('prod') {
  addJob('job1', 'job1.1') {
    script = "src/main/pig/count_by_country.job1.1"
  }
  depends 'job1.1'

  pigJob('prodJob') {
    script = 'workflowProdJob'
  }
}

pigJob('job20') {
  script = 'src/main/pig/' + 'count_by_country.job'
  parameter 'member_summary', '/data/derived/member/summary#LATEST'
  parameter 'output', 'count_by_country.dat'
  reads '/data/databases/foo', [as: 'input']
  writes '/data/databases/bar', [as: 'output']
}

propertySet('global') {
  myPropertyA = 'globalA'
  myPropertyB = 'globalB'
}

azkaban {
  // Directory in which to generate the job files
  jobConfDir = "${project.projectDir}/jobs"

  propertySet('common') {
    myPropertyA = 'valA'
    myPropertyB = 'valB'
  }

  addPropertySet('global') {
    myPropertyB = 'globalBUpdate'
    myPropertyC = 'globalC'
  }

  generateWorkflows()

  workflow('workflow1') {
    // Add a job to the workflow that you defined globally. You can add it to
    // several workflows, and each will get a job file scoped to that workflow.
    addJob('job20') {
      parameter 'output', 'count_by_country.dat'
      reads '/data/databases/foo', [as: 'input']
    }

    def job20 = lookup('job20')
    job20.script = 'src/main/pig/count_by_country.job20'

    def job21 = new PigJob('job21')
    job21.script = 'foobar'
    local job21

    lookup('job20') {
      writes '/data/databases/bar', [as: 'output']
    }

    pigJob('job1') {
      script = "foobars"
    }

    azkabanJob('job2') {
      reads '/data/databases/foo', [as: 'input']
      writes '/data/databases/bar', [as: 'output']
      caches '/data/databases/bazz', [as: 'foobarbazz']
      setJob 'propertyName1', 'propertyValue1'
      setJvm 'udf.import.list', 'oink.,com.linkedin.pig.'
      depends 'job1'
    }

    azkabanJob('job3') {
    }

    azkabanJob('job4') {
      depends 'job3'
    }

    azkabanJob('job5') {
      depends 'job4'
    }

    depends 'job2', 'job3', 'job5', 'job20'

    propertySet('workflow1-common') {
      myPropertyA = 'valA.1'
      myPropertyB = 'valB.1'
    }
  }

  local train
}

azkaban.workflow('workflow2') {
  pigJob('job6') {
    script = 'src/main/pig/' + 'count_by_country.job'
    parameter 'member_summary', '/data/derived/member/summary#LATEST'
    parameter 'output', 'count_by_country.dat'
    reads '/data/databases/foo', [as: 'input']
    writes '/data/databases/bar', [as: 'output']
  }

  hiveJob('job7') {
    query = "show tables"
  }

  commandJob('job8') {
    uses 'echo "hello world"'
  }

  javaJob('job9') {
    uses "com.linkedin.hello.World"
  }

  javaProcessJob('job10') {
    uses "com.linkedin.hello.World"
  }

  voldemortBuildPushJob('job11') {
  }

  depends 'job11', 'job10', 'job9', 'job8', 'job7', 'job6'

  if (false) {
    pigJob('job12') {
      script = 'src/main/pig/count_by_country2.job'
    }
    depends 'job12'
  }
  else {
    pigJob('job12Else') {
      if (false) {
        script = 'src/main/pig/count_by_country2.job'
      }
      else {
        script = 'src/main/pig/count_by_country2else.job'
      }
    }
    depends 'job12Else'
  }
}