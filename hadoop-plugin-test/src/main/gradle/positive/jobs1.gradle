buildscript {
  dependencies {
    classpath files("${project.pluginTestDir}/hadoop-plugin-${project.libVersion}.jar")
  }
}

apply plugin: com.linkedin.gradle.hadoop.HadoopPlugin

// Simple positive test cases for the various types of jobs.

// Declare a couple jobs in global scope to check that these functions are working.

pigJob('pigJob1') {
  uses 'src/main/pig/pigScript.pig'
  reads files: [
    'path1' : '/user/foo'
  ]
  writes files: [
    'path2' : '/user/bar'
  ]
  set parameters: [
    'param1' : 'val1'
  ]
}

noOpJob('noOpJob1') {
  depends 'pigJob1'
}

// Now declare jobs for each of the job types to test them all out.

hadoop {
  buildPath "jobs"
  cleanPath false

  // Should not be built as this job is not within a workflow
  pigJob('pigJob0') {
    uses 'src/main/pig/pigScript.pig'
  }

  workflow('jobs1') {
    job('job1') {
    }

    job('job2') {
      reads files: [
        'foo' : '/data/databases/foo',
        'bar' : '/data/databases/bar',
      ]
      writes files: [
        'bazz' : '/data/databases/bazz'
      ]
      set properties: [
        'propertyName1' : 'propertyValue1'
      ]
      depends 'job1'
    }

    commandJob('job3') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job2']
    }

    hadoopJavaJob('job4') {
      uses 'com.linkedin.hello.HadoopJavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      depends 'job3'
    }

    hiveJob('job5') {
      uses "hello.q"
      reads files: [
        'path1' : '/user/foo'
      ]
      writes files: [
        'path2' : '/user/bar'
      ]
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set parameters: [
        'param1': 'val1',
        'param2': 'val2'
      ]
      queue 'marathon'
      depends 'job4'
    }

    javaJob('job6') {
      uses 'com.linkedin.hello.JavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096
      ]
      // Test using both set confProperties and set hadoopProperties at the same time
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      // Test using the list syntax for the job depends method. This requires the use of parens.
      depends(['job5'])
    }

    javaProcessJob('job7') {
      uses 'com.linkedin.hello.JavaProcessJob'
      jvmClasspath './lib/*'
      set jvmProperties: [
        'jvmPropertyName1' : 'jvmPropertyValue1'
      ]
      Xms 128
      Xmx 1024
      depends 'job6'
    }

    kafkaPushJob('job8') {
      usesInputPath '/data/databases/MEMBER2/MEMBER_PROFILE/#LATEST'  // Required
      usesTopic 'kafkatestpush'                                       // Required
      usesBatchNumBytes 1000000                                       // Optional
      usesDisableSchemaRegistration true                              // Optional
      usesKafkaUrl 'eat1-ei2-kafka-vip-c.stg.linkedin.com:10251'      // Optional
      usesNameNode 'hdfs://eat1-magicnn01.grid.linkedin.com:9000'     // Optional
      usesSchemaRegistryUrl 'http://eat1-app501:10252/schemaRegistry/schemas'  // Optional
      usesDisableAuditing true                                        // Optional
      depends 'job7'
      queue 'marathon'
    }

    noOpJob('job9') {
      depends 'job8'
    }

    pigJob('job10') {
      uses 'src/main/pig/pigScript.pig'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set parameters: [
        'param1' : 'val1'
      ]
      queue 'marathon'
      depends 'job9'
    }

    sparkJob('job11') {
      uses 'com.linkedin.hello.spark.HelloSpark' // required
      executes 'hello-spark.jar' // required

      def params = ['param1','param2']
      def flags = ['verbose','version']

      appParams params // optional
      enableFlags flags // optional

      set sparkConfs: [
        'key1': 'value1',
        'key2': 'value2'
      ]

      jars 'jar1,jar2,jar3'
      numExecutors 120
      executorMemory '2g'
      driverMemory '2g'
      executorCores 1

      set properties: [
        'master':'yarn-cluster',
      ]
      set jvmProperties: [
        'log4j.configuration' : 'log.properties'
      ]
      queue 'marathon'
      depends 'job10'
    }

    voldemortBuildPushJob('job12') {
      usesStoreName 'test-store'          // Required
      usesClusterName 'tcp://foo:10103'   // Required
      usesInputPath '/user/foo/input'     // Required
      usesOutputPath '/user/foo/output'   // Required
      usesStoreOwners 'foo@linkedin.com'  // Required
      usesStoreDesc 'Store for testing'   // Required
      usesTempDir '/tmp/foo'              // Optional
      usesRepFactor 2                     // Optional
      usesCompressValue false             // Optional
      usesKeySelection 'memberId'         // Optional
      usesValueSelection 'lastName'       // Optional
      usesNumChunks(-1)                   // Optional
      usesChunkSize 1073741824            // Optional
      usesKeepOutput false                // Optional
      usesPushHttpTimeoutSeconds 86400    // Optional
      usesPushNode 0                      // Optional
      usesBuildStore true                 // Optional
      usesPushStore true                  // Optional
      usesFetcherProtocol 'hftp'          // Optional
      usesFetcherPort '50070'             // Optional
      usesAvroSerializerVersioned false   // Optional
      usesAvroData true                   // Optional
      usesAvroKeyField 'memberId'         // Optional unless isAvroData is true
      usesAvroValueField 'firstName'      // Optional unless isAvroData is true
      depends 'job11'
      queue 'marathon'
    }

    hadoopShellJob('job13') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job12']
    }

    hdfsToTeradataJob('job14') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      credentialName 'com.linkedin.teradata.scott'       //Required
      sourceHdfsPath '/data/test/users/daily/2015-10-07' //Required
      targetTable 'dwh_stg.scott_users'                  //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott"
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job13'
    }

    teradataToHdfsJob('job15') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      credentialName 'com.linkedin.teradata.scott'       //Required
      sourceTable 'dwh_stg.scott_users'                //Required
      targetHdfsPath '/data/test/users/teradata_to_hdfs_test' //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott",
        "force.output.overwrite": true
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job14'
    }

    hdfsToEspressoJob('job16') {
      qps 1000                                                               //Required
      sourceHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/shortest' //Required
      espressoEndpoint 'http://eat1-app1237.stg.linkedin.com:11936'          //Required
      espressoDatabaseName 'MerlinAccounts'                                  //Required
      espressoTableName 'AccountMetric'                                      //Required
      espressoContentType 'APPLICATION_JSON'                                 //Required
      espressoKey 'accountId'                                                //Required
      espressoSubkeys 'metricName'                                           //Optional
      espressoOperation 'put'                                                //Required
      errorHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/error'     //Required
      set properties: [                                                      //Optional
        'force.error.overwrite' : true
      ]
      depends 'job15'
    }

    gobblinJob('job17') {
      workDir '/jobs/jnchang/azkaban/gobblin'  //Optional
      preset 'mysqlToHdfs'                     //Optional
      set properties: [                        //Optional
        'source.querybased.schema' : 'merlin_users',
        'source.entity' : 'user',
        'source.conn.host' : 'eat1-db42.corp.linkedin.com',
        'source.conn.username' : 'merlin_u',
        'source.conn.password' : 'ENC(7Bv+t0aw5rdyOYuyLqkrpTetE3KvfDRK)',
        'encrypt.key.loc' : '/jobs/jnchang/mstr_merlin.key',
        'extract.table.type' : 'snapshot_only',
        'extract.is.full' : true,
        'data.publisher.replace.final.dir' : true,
        'data.publisher.final.dir' :  '${gobblin.work_dir}/job-output'
      ]
      depends 'job16'
    }

    targets 'job17'
  }
}
